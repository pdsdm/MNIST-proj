{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIVE MNIST \n",
    "\n",
    "The LIVE MNIST project aims to create an interactive and dynamic visualization of the training, validation, and testing process for the well-known MNIST dataset. Users will be able to configure the number of layers and nodes in a Convolutional Neural Network (CNN) and observe a live representation of the model’s structure and learning progress directly on a web interface. Additionally, the project will feature an interactive drawing tool where users can sketch digits in real-time and instantly see the model’s predicted probabilities for each number, making the experience both educational and engaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) MODEL rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (3.9.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/pepmoyanofont/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are the directions of our training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = \"dataset/train-images.idx3-ubyte\"\n",
    "train_y = \"dataset/train-labels.idx1-ubyte\"\n",
    "test_x = \"dataset/t10k-images.idx3-ubyte\"\n",
    "test_y = \"dataset/t10k-labels.idx1-ubyte\"\n",
    "\n",
    "train_csv = \"./csv_files/train.csv\"\n",
    "test_csv = \"./csv_files/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert the ubyte into csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert (imgs, labels, outline, n):\n",
    "    imgf = open(imgs, 'rb') #this function opens the ubyte file.\n",
    "    labelf = open(labels, 'rb')\n",
    "    csvf = open(outline, 'w')\n",
    "\n",
    "    imgf.read(16) #Jumping few bytes due to metadata\n",
    "    labelf.read(8)\n",
    "    images = []\n",
    "\n",
    "    for i in range(n):\n",
    "        image = [ord(labelf.read(1))] #The reason is 1 is because you want to read just 1 byte. When readed, it automatically jumps to next byte.\n",
    "        for j in range(28*28):\n",
    "            image.append(ord(imgf.read(1))) #When appending single num, we added to the list we are in.\n",
    "        images.append(image)  #Finally adding the existing list to another one.\n",
    "    \n",
    "    for image in images:\n",
    "        csvf.write(\",\".join(str(plx) for plx in image)+ \"\\n\") #The conversion into csv file.\n",
    "\n",
    "    imgf.close()\n",
    "    labelf.close()\n",
    "    csvf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert(train_x, train_y, train_csv, 60000) #generating the files\n",
    "convert(test_x, test_y, test_csv, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "train_file = open(train_csv, 'r')  \n",
    "train_list = train_file.readlines()\n",
    "train_file.close()\n",
    "print(len(train_list))\n",
    "num = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,38,43,105,255,253,253,253,253,253,174,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,43,139,224,226,252,253,252,252,252,252,252,252,158,14,0,0,0,0,0,0,0,0,0,0,0,0,0,0,178,252,252,252,252,253,252,252,252,252,252,252,252,59,0,0,0,0,0,0,0,0,0,0,0,0,0,0,109,252,252,230,132,133,132,132,189,252,252,252,252,59,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,29,29,24,0,0,0,0,14,226,252,252,172,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,85,243,252,252,144,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,88,189,252,252,252,14,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,91,212,247,252,252,252,204,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,32,125,193,193,193,253,252,252,252,238,102,28,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,45,222,252,252,252,252,253,252,252,252,177,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,45,223,253,253,253,253,255,253,253,253,253,74,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,31,123,52,44,44,44,44,143,252,252,74,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,252,252,74,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,86,252,252,74,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,75,9,0,0,0,0,0,0,98,242,252,252,74,0,0,0,0,0,0,0,0,0,0,0,0,0,61,183,252,29,0,0,0,0,18,92,239,252,252,243,65,0,0,0,0,0,0,0,0,0,0,0,0,0,208,252,252,147,134,134,134,134,203,253,252,252,188,83,0,0,0,0,0,0,0,0,0,0,0,0,0,0,208,252,252,252,252,252,252,252,252,253,230,153,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,49,157,252,252,252,252,252,217,207,146,45,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,103,235,252,172,103,24,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\\n'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "values = train_list[num].split(\",\")\n",
    "images_array = np.asarray(values[1:], dtype=float).reshape((28,28))\n",
    "plt.imshow(images_array, cmap = \"Grays\", interpolation = \"None\")\n",
    "plt.grid(True, \"major\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0   0   0  38  43 105 255 253 253 253 253 253 174   6  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0  43 139 224 226 252 253 252 252 252 252 252 252 158 14 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0 178 252 252 252 252 253 252 252 252 252 252 252 252 59 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0 109 252 252 230 132 133 132 132 189 252 252 252 252 59 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0   4  29  29  24   0   0   0   0  14 226 252 252 172  7 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0   0   0   0   0   0   0   0   0  85 243 252 252 144  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0   0   0   0   0   0   0   0  88 189 252 252 252  14  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0   0   0   0   0   0  91 212 247 252 252 252 204   9  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0  32 125 193 193 193 253 252 252 252 238 102  28   0  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0  45 222 252 252 252 252 253 252 252 252 177   0   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0  45 223 253 253 253 253 255 253 253 253 253  74   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0  31 123  52  44  44  44  44 143 252 252  74   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0   0   0   0   0   0   0   0  15 252 252  74   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0   0   0   0   0   0   0   0  86 252 252  74   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   5  75   9   0   0   0   0   0   0  98 242 252 252  74   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0  61 183 252  29   0   0   0   0  18  92 239 252 252 243  65   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0 208 252 252 147 134 134 134 134 203 253 252 252 188  83   0   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0 208 252 252 252 252 252 252 252 252 253 230 153   8   0   0   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0  49 157 252 252 252 252 252 217 207 146  45   0   0   0   0   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   7 103 235 252 172 103  24   0   0   0   0   0   0   0   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  0 0 0 0 0 0\n",
      "0 0 0 0 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "image_matrix = images_array.astype(int)\n",
    "print(pd.DataFrame(image_matrix).to_string(index=False, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "test_file = open(test_csv, 'r')\n",
    "test_list = test_file.readlines()\n",
    "test_file.close()\n",
    "print(len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, sizes, epochs, lr):\n",
    "        self.sizes = sizes #This are the sizes of our neuron\n",
    "        self.epochs = epochs #Number of epochs\n",
    "        self.lr = lr #Is the learning rate which control how much to adjust weights in reponse tot the gradient of the loss function\n",
    "\n",
    "        input_layer = sizes[0]\n",
    "        hidden_1 = sizes[1]\n",
    "        hidden_2 = sizes[2]\n",
    "        output_layer = sizes[3]\n",
    "\n",
    "        self.params = {\n",
    "            'W1':np.random.randn(hidden_1, input_layer) * np.sqrt(1./hidden_1),         #128x784, this creates random weights that are connected to our nodes. At the start is better to have this random weights.\n",
    "            'W2':np.random.randn(hidden_2, hidden_1) * np.sqrt(1./hidden_2),            #64x784\n",
    "            'W3':np.random.randn(output_layer, hidden_2 ) * np.sqrt(1./output_layer)    #10x64\n",
    "        }\n",
    "\n",
    "    def sigmoid(self, x, derivative=False): #This function is used to ensure proper values between -1 and 1. We do it like this, becuase everyone does it. Investigate if there is a better function for converting this number.\n",
    "        if derivative:\n",
    "            return (np.exp(-x))/(np.exp(x)**2)  #sigmoid itself. Consider using ReLu??? Maybe to simple (linear and [0, 1]\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self, x, derivative=False): #This function is the one we use for the output. Becuase the output, gives us what looks like random numbers, we take the sum of it, and we plot them into values between [0 and 1] with the higuer value (<1) beign the most probable output.\n",
    "        exps = np.exp(x-np.max(x))\n",
    "        if derivative:\n",
    "            return exps/np.sum(exps, axis=0) * (1-exps / np.sum(exps, axis=0))\n",
    "        return exps/np.sum(exps, axis=0)\n",
    "\n",
    "\n",
    "    def forward_pass(self, x_train): #First step of the taining and also to use it. We forward the values and it gives us the probable cause.\n",
    "        params = self.params\n",
    "        params['A0'] = x_train\n",
    "\n",
    "        params['Z1'] = np.dot(params['W1'], params['A0']) #Here we take the values of train (between 0 to 255) and product multiply by the random weights (just in first iteration)\n",
    "        params['A1'] = self.sigmoid(params['Z1']) #Then we take the result of the last and leave it between values of -1 and 1.\n",
    "\n",
    "        params['Z2'] = np.dot(params['W2'], params['A1'])\n",
    "        params['A2'] = self.sigmoid(params['Z2'])\n",
    "\n",
    "        params['Z3'] = np.dot(params['W3'], params['A2'])\n",
    "        params['A3'] = self.softmax(params['Z3']) #Note here the softmax; values from 0 to 1. The final probability.\n",
    "\n",
    "        return params['A3']\n",
    "    \n",
    "\n",
    "    def backward_pass(self, y_train, output): # In training we use this backward with the forwrd in order to alterate the values.\n",
    "        params = self.params\n",
    "        change_w = {}\n",
    "\n",
    "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], derivative = True)\n",
    "        change_w['W3'] = np.outer(error, params['A2'])\n",
    "\n",
    "        error = np.dot(params['W3'].T, error) * self.sigmoid(params['Z2'], derivative = True)\n",
    "        change_w['W2'] = np.outer(error, params['A1'])\n",
    "\n",
    "        error = np.dot(params['W2'].T, error) * self.sigmoid(params['Z1'], derivative = True)\n",
    "        change_w['W1'] = np.outer(error, params['A0'])\n",
    "\n",
    "        return change_w\n",
    "    \n",
    "    def update_weights(self, change_w):\n",
    "        for key, val in change_w.items():\n",
    "            self.params[key] -= self.lr * val\n",
    "\n",
    "    \n",
    "    def compute_accuracy(self, test_list):\n",
    "        predictions = []\n",
    "        for x in test_list:\n",
    "            values = x.split(',')\n",
    "            inputs = (np.asarray(values[1:], dtype=np.float32) / 255.0 * 0.99) + 0.01\n",
    "            targets = np.zeros(10) + 0.01\n",
    "            targets[int(values[0])] = 0.99\n",
    "            output = self.forward_pass(inputs)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred==np.argmax(targets))\n",
    "\n",
    "        return np.mean(predictions)\n",
    "\n",
    "\n",
    "    def train(self, train_list, test_list):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for iter in range(self.epochs):\n",
    "            for x in train_list:\n",
    "                values = x.split(',')\n",
    "                inputs = (np.asarray(values[1:], dtype=np.float32) / 255.0 * 0.99) + 0.01\n",
    "                targets = np.zeros(10) + 0.01\n",
    "                targets[int(values[0])] = 0.99\n",
    "                output = self.forward_pass(inputs)\n",
    "                change_w = self.backward_pass(targets, output)\n",
    "                self.update_weights(change_w)\n",
    "\n",
    "            accuracy = self.compute_accuracy(test_list)\n",
    "\n",
    "            print('Epoch: {0}, Time Spent: {1:.02f}s, Accuracy: {2:.2f}%'.format(iter+1, time.time()-start_time, accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, sizes, epochs, lr):\n",
    "        self.sizes = sizes  # Tamaños de las capas\n",
    "        self.epochs = epochs  # Número de épocas\n",
    "        self.lr = lr  # Tasa de aprendizaje\n",
    "\n",
    "        input_layer = sizes[0]\n",
    "        hidden_1 = sizes[1]\n",
    "        hidden_2 = sizes[2]\n",
    "        output_layer = sizes[3]\n",
    "\n",
    "        self.params = {\n",
    "            'W1': np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
    "            'W2': np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
    "            'W3': np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
    "        }\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        sig = 1 / (1 + np.exp(-x))\n",
    "        if derivative:\n",
    "            return sig * (1 - sig)\n",
    "        return sig\n",
    "    \n",
    "    def softmax(self, x, derivative=False):\n",
    "        exps = np.exp(x - np.max(x))  # Evitar overflow\n",
    "        softmax_vals = exps / np.sum(exps, axis=0)\n",
    "        if derivative:\n",
    "            return softmax_vals * (1 - softmax_vals)\n",
    "        return softmax_vals\n",
    "\n",
    "    def forward_pass(self, x_train):\n",
    "        params = self.params\n",
    "        params['A0'] = x_train\n",
    "\n",
    "        params['Z1'] = np.dot(params['W1'], params['A0'])\n",
    "        params['A1'] = self.sigmoid(params['Z1'])\n",
    "\n",
    "        params['Z2'] = np.dot(params['W2'], params['A1'])\n",
    "        params['A2'] = self.sigmoid(params['Z2'])\n",
    "\n",
    "        params['Z3'] = np.dot(params['W3'], params['A2'])\n",
    "        params['A3'] = self.softmax(params['Z3'])\n",
    "\n",
    "        return params['A3']\n",
    "    \n",
    "    def backward_pass(self, y_train, output):\n",
    "        params = self.params\n",
    "        change_w = {}\n",
    "\n",
    "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], derivative=True)\n",
    "        change_w['W3'] = np.outer(error, params['A2'])\n",
    "\n",
    "        error = np.dot(params['W3'].T, error) * self.sigmoid(params['Z2'], derivative=True)\n",
    "        change_w['W2'] = np.outer(error, params['A1'])\n",
    "\n",
    "        error = np.dot(params['W2'].T, error) * self.sigmoid(params['Z1'], derivative=True)\n",
    "        change_w['W1'] = np.outer(error, params['A0'])\n",
    "\n",
    "        return change_w\n",
    "    \n",
    "    def update_weights(self, change_w):\n",
    "        for key, val in change_w.items():\n",
    "            if self.params[key].shape == val.shape:\n",
    "                self.params[key] -= self.lr * val\n",
    "\n",
    "    def compute_accuracy(self, test_list):\n",
    "        predictions = []\n",
    "        for x in test_list:\n",
    "            values = x.split(',')\n",
    "            inputs = (np.asarray(values[1:], dtype=np.float32) / 255.0 * 0.99) + 0.01\n",
    "            targets = np.zeros(10) + 0.01\n",
    "            targets[int(values[0])] = 0.99\n",
    "            output = self.forward_pass(inputs)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred == np.argmax(targets))\n",
    "\n",
    "        return np.mean(predictions)\n",
    "\n",
    "    def train(self, train_list, test_list):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for iter in range(self.epochs):\n",
    "            for x in train_list:\n",
    "                values = x.split(',')\n",
    "                inputs = (np.asarray(values[1:], dtype=np.float32) / 255.0 * 0.99) + 0.01\n",
    "                targets = np.zeros(10) + 0.01\n",
    "                targets[int(values[0])] = 0.99\n",
    "                output = self.forward_pass(inputs)\n",
    "                change_w = self.backward_pass(targets, output)\n",
    "                self.update_weights(change_w)\n",
    "\n",
    "            accuracy = self.compute_accuracy(test_list)\n",
    "            print(f'Epoch: {iter+1}, Time Spent: {time.time()-start_time:.02f}s, Accuracy: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time Spent: 10.97s, Accuracy: 69.36%\n",
      "Epoch: 2, Time Spent: 21.81s, Accuracy: 79.58%\n",
      "Epoch: 3, Time Spent: 32.65s, Accuracy: 85.67%\n",
      "Epoch: 4, Time Spent: 43.73s, Accuracy: 87.87%\n",
      "Epoch: 5, Time Spent: 54.70s, Accuracy: 89.37%\n",
      "Epoch: 6, Time Spent: 65.51s, Accuracy: 90.31%\n",
      "Epoch: 7, Time Spent: 76.36s, Accuracy: 91.16%\n",
      "Epoch: 8, Time Spent: 87.10s, Accuracy: 91.67%\n",
      "Epoch: 9, Time Spent: 97.96s, Accuracy: 92.02%\n",
      "Epoch: 10, Time Spent: 108.86s, Accuracy: 92.41%\n"
     ]
    }
   ],
   "source": [
    "dnn = CNN(sizes = [784, 128, 64, 10], epochs = 10, lr = 0.03)\n",
    "dnn.train(train_list, test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achived an accuraccy of 92.41% which is quite fine but, we could improve this by inserting a decaying learning rate. At the star, this will make big changes on the weights and each epoch will have a decrese on the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_decay:\n",
    "    def __init__(self, sizes, epochs, lr, decay=0.0):\n",
    "        self.sizes = sizes \n",
    "        self.epochs = epochs  \n",
    "        self.lr = lr \n",
    "        self.decay = decay \n",
    "\n",
    "        input_layer = sizes[0]\n",
    "        hidden_1 = sizes[1]\n",
    "        hidden_2 = sizes[2]\n",
    "        output_layer = sizes[3]\n",
    "\n",
    "        self.params = {\n",
    "            'W1': np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),\n",
    "            'W2': np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),\n",
    "            'W3': np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)\n",
    "        }\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        sig = 1 / (1 + np.exp(-x))\n",
    "        if derivative:\n",
    "            return sig * (1 - sig)\n",
    "        return sig\n",
    "    \n",
    "    def softmax(self, x, derivative=False):\n",
    "        exps = np.exp(x - np.max(x))  \n",
    "        softmax_vals = exps / np.sum(exps, axis=0)\n",
    "        if derivative:\n",
    "            return softmax_vals * (1 - softmax_vals)\n",
    "        return softmax_vals\n",
    "\n",
    "    def forward_pass(self, x_train):\n",
    "        params = self.params\n",
    "        params['A0'] = x_train\n",
    "\n",
    "        params['Z1'] = np.dot(params['W1'], params['A0'])\n",
    "        params['A1'] = self.sigmoid(params['Z1'])\n",
    "\n",
    "        params['Z2'] = np.dot(params['W2'], params['A1'])\n",
    "        params['A2'] = self.sigmoid(params['Z2'])\n",
    "\n",
    "        params['Z3'] = np.dot(params['W3'], params['A2'])\n",
    "        params['A3'] = self.softmax(params['Z3'])\n",
    "\n",
    "        return params['A3']\n",
    "    \n",
    "    def backward_pass(self, y_train, output):\n",
    "        params = self.params\n",
    "        change_w = {}\n",
    "\n",
    "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], derivative=True)\n",
    "        change_w['W3'] = np.outer(error, params['A2'])\n",
    "\n",
    "        error = np.dot(params['W3'].T, error) * self.sigmoid(params['Z2'], derivative=True)\n",
    "        change_w['W2'] = np.outer(error, params['A1'])\n",
    "\n",
    "        error = np.dot(params['W2'].T, error) * self.sigmoid(params['Z1'], derivative=True)\n",
    "        change_w['W1'] = np.outer(error, params['A0'])\n",
    "\n",
    "        return change_w\n",
    "    \n",
    "    def update_weights(self, change_w):\n",
    "        for key, val in change_w.items():\n",
    "            if self.params[key].shape == val.shape:\n",
    "                self.params[key] -= self.lr * val\n",
    "\n",
    "    def compute_accuracy(self, test_list):\n",
    "        predictions = []\n",
    "        for x in test_list:\n",
    "            values = x.split(',')\n",
    "            inputs = (np.asarray(values[1:], dtype=np.float32) / 255.0 * 0.99) + 0.01\n",
    "            targets = np.zeros(10) + 0.01\n",
    "            targets[int(values[0])] = 0.99\n",
    "            output = self.forward_pass(inputs)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred == np.argmax(targets))\n",
    "\n",
    "        return np.mean(predictions)\n",
    "\n",
    "    def train(self, train_list, test_list):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for iter in range(self.epochs):\n",
    "            for x in train_list:\n",
    "                values = x.split(',')\n",
    "                inputs = (np.asarray(values[1:], dtype=np.float32) / 255.0 * 0.99) + 0.01\n",
    "                targets = np.zeros(10) + 0.01\n",
    "                targets[int(values[0])] = 0.99\n",
    "                output = self.forward_pass(inputs)\n",
    "                change_w = self.backward_pass(targets, output)\n",
    "                self.update_weights(change_w)\n",
    "            \n",
    "            self.lr *= (1. / (1. + self.decay * iter))\n",
    "            \n",
    "            accuracy = self.compute_accuracy(test_list)\n",
    "            print(f'Epoch: {iter+1}, Time Spent: {time.time()-start_time:.02f}s, Accuracy: {accuracy*100:.2f}%, Learning Rate: {self.lr:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time Spent: 11.12s, Accuracy: 93.88%, Learning Rate: 0.900000\n",
      "Epoch: 2, Time Spent: 22.25s, Accuracy: 95.07%, Learning Rate: 0.849057\n",
      "Epoch: 3, Time Spent: 33.43s, Accuracy: 96.32%, Learning Rate: 0.758086\n",
      "Epoch: 4, Time Spent: 44.59s, Accuracy: 96.93%, Learning Rate: 0.642446\n",
      "Epoch: 5, Time Spent: 55.67s, Accuracy: 97.06%, Learning Rate: 0.518102\n",
      "Epoch: 6, Time Spent: 66.74s, Accuracy: 97.45%, Learning Rate: 0.398540\n",
      "Epoch: 7, Time Spent: 78.03s, Accuracy: 97.47%, Learning Rate: 0.293044\n",
      "Epoch: 8, Time Spent: 89.44s, Accuracy: 97.54%, Learning Rate: 0.206369\n",
      "Epoch: 9, Time Spent: 100.60s, Accuracy: 97.49%, Learning Rate: 0.139438\n",
      "Epoch: 10, Time Spent: 111.66s, Accuracy: 97.47%, Learning Rate: 0.090544\n"
     ]
    }
   ],
   "source": [
    "dnn2 = CNN_decay(sizes = [784, 128, 64, 10], epochs = 10, lr = 0.9, decay = 0.06)\n",
    "dnn2.train(train_list, test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time Spent: 10.89s, Accuracy: 95.17%, Learning Rate: 2.000000\n",
      "Epoch: 2, Time Spent: 21.86s, Accuracy: 96.01%, Learning Rate: 1.818182\n",
      "Epoch: 3, Time Spent: 32.67s, Accuracy: 96.58%, Learning Rate: 1.515152\n",
      "Epoch: 4, Time Spent: 43.39s, Accuracy: 96.98%, Learning Rate: 1.165501\n",
      "Epoch: 5, Time Spent: 54.15s, Accuracy: 97.37%, Learning Rate: 0.832501\n",
      "Epoch: 6, Time Spent: 64.95s, Accuracy: 97.69%, Learning Rate: 0.555001\n",
      "Epoch: 7, Time Spent: 75.75s, Accuracy: 97.71%, Learning Rate: 0.346875\n",
      "Epoch: 8, Time Spent: 86.55s, Accuracy: 97.85%, Learning Rate: 0.204044\n",
      "Epoch: 9, Time Spent: 97.35s, Accuracy: 97.81%, Learning Rate: 0.113358\n",
      "Epoch: 10, Time Spent: 108.14s, Accuracy: 97.82%, Learning Rate: 0.059662\n"
     ]
    }
   ],
   "source": [
    "dnn3 = CNN_decay(sizes = [784, 128, 64, 10], epochs = 10, lr = 2, decay = 0.1)\n",
    "dnn3.train(train_list, test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
